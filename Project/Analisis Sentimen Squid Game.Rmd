---
title: "Analisis Sentimen Squid Game"
author: "Gisani MR"
date: "12/8/2021"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
library(twitteR)

```

```{r}


api_key<- "cvE55mBpAA4ryBCO2QFvkkrA3"
api_secret<- "lyCrqoYeGj1zmkl25Nn9DBgNFjXpUzEbRksT3LpdxP0jnxsz8q"
access_token<- "1462997508859187204-9AiHfSDm17ogce6xs3VlNkrSv9utHK"
access_token_secret<- "CkuYYSOajxV7fbh7pTZYFHlrLEz8kWo5q3hkggcVZkws3"
setup_twitter_oauth(api_key,api_secret,access_token,access_token_secret)


tw = searchTwitter('squid game -filter:retweets', 
                   n = 10000,
                   retryOnRateLimit = 10e5, 
                   lang = "id" ) #retryOnRateLimit untuk looping
saveRDS(tw,file = 'tweetSquidGame.rds')

datatori <- do.call("rbind", lapply(tw, as.data.frame))

write.csv(datatori,'OriginalTweetSquidGame.csv')


```

```{r}
library(vroom) #membaca data
library(tm) #untuk cleaning text data
library(tidyverse)
```


```{r}

#CLEANING DATA

tw <- readRDS('tweetSquidGame.rds')
DataKotor = twListToDF(tw) #convert twitteR list to data

#menampilkan semua tweet yang kita mining
DataKotor2 <- DataKotor$text

DataKotorCorpus <- Corpus(VectorSource(DataKotor2))


##hapus URL
removeURL <- function(x) gsub("http[^[:space:]]*", "", x)
twitclean <- tm_map(DataKotorCorpus, removeURL)

##hapus New Line
removeNL <- function(y) gsub("\n", "", y)
twitclean <- tm_map(twitclean, removeNL)

##hapus koma
replacecomma <- function(y) gsub(",", "", y)
twitclean <- tm_map(twitclean, replacecomma)

##hapus retweet
removeRT <- function(y) gsub("RT ", "", y)
twitclean <- tm_map(twitclean, removeRT)

##hapus titik
removetitik2 <- function(y) gsub(":", "", y)
twitclean <- tm_map(twitclean, removetitik2)

##hapus titik koma
removetitikkoma <- function(y) gsub(";", " ", y)
twitclean <- tm_map(twitclean, removetitikkoma)

#hapus titik3
removetitik3 <- function(y) gsub("p.", "", y)
twitclean <- tm_map(twitclean, removetitik3)

#hapus &amp
removeamp <- function(y) gsub("&amp;", "", y)
twitclean <- tm_map(twitclean, removeamp)

#hapus Mention
removeUN <- function(z) gsub("@\\w+", "", z)
twitclean <- tm_map(twitclean, removeUN)

#hapus space dll
remove.all <- function(xy) gsub("[^[:alpha:][:space:]]*", "", xy)
twitclean <-tm_map(twitclean,stripWhitespace)
inspect(twitclean[1:10])
twitclean <- tm_map(twitclean,remove.all)
twitclean <- tm_map(twitclean, removePunctuation) #tanda baca
twitclean <- tm_map(twitclean, tolower) #mengubah huruf kecil
myStopwords <- readLines("stopword.txt", warn = FALSE)
twitclean <- tm_map(twitclean,removeWords,myStopwords)
twitclean <- tm_map(twitclean , removeWords, 
                    c('kalo','akun','ada','sini','langsung','','gak','org','saya','nonton','bayu',
                      'udah','sekarang','tengok','isteri','netflix','tinggal','korea','udin','itu','dap',
                      'premiumanti','squid','aja','kan','dah','kayak','dari','gue','kemarin','bahan','kyk','tinggal','yang','game','taehyung','banget','tae','dijaminprofil','nya',
                      'ridiii','kakaa','coss','sudah','skak','dahal','jadi','kkalau','sebab','mesti','bgt',
                      'nak','gw','yg','sama','bisa',''))



#HAPUS DATA KOSONG
try.error = function(x)
{
  # create missing value
  y = NA
  # tryCatch error
  try_error = tryCatch(tolower(x), error=function(e) e)
  # if not an error
  if (!inherits(try_error, "error"))
    y = tolower(x)
  # result
  return(y)
}

# lower case using try.error with sapply 
twitclean = sapply(twitclean, try.error)

# remove NAs in some_txt
twitclean = twitclean[!is.na(twitclean)]
names(twitclean) = NULL


# dataframe data yg sudah bersih
dataframe<-data.frame(text=unlist(sapply(twitclean, `[`)), stringsAsFactors=F)
View(dataframe)
write.csv(dataframe,'TweetCleanSquidGame.csv')


```

```{r}
library(e1071) #untuk naive bayes
library(caret) #untuk klasifikasi data
library(syuzhet) #untuk membaca fungsi get_nrc
```

```{r}

tweetDataClean <- read.csv("TweetCleanSquidGame.csv",stringsAsFactors = FALSE)
twtClean <- as.character(tweetDataClean$text) #merubah text menjadi char
s <- get_nrc_sentiment(twtClean)

twt_combine <- cbind(tweetDataClean$text,s) #klasifikasi data
par(mar=rep(3,4))
a <- barplot(colSums(s),col=rainbow(10),ylab='count',main='Sentiment Analisis Squid Game')
brplt <- a

a

```

```{r}
#library untuk penggunaan corpus dalam cleaning data
library(tm) #untuk cleaning text data
library(RTextTools)#mengkalisifikasi text secara otomatis dengan supervised learning
#library yang terdapat sebuah algoritma naivebayes
library(e1071)
library(dplyr)
library(caret)
df<-read.csv("TweetCleanSquidGame.csv",stringsAsFactors = FALSE)
glimpse(df)

#Set the seed of Râ€˜s random number generator, which is useful for creating simulations or random objects that can be reproduced.
set.seed(20)
df<-df[sample(nrow(df)),]
df<-df[sample(nrow(df)),]
glimpse(df)

mystopword<-readLines('stopword.txt')

corpus<-Corpus(VectorSource(df$text))
corpus
inspect(corpus[1:10])
#fungsinya untuk membersihkan data data yang tidak dibutuhkan 
corpus.clean<-corpus%>%
    tm_map(content_transformer(tolower))%>%
    tm_map(removePunctuation)%>%
    tm_map(removeNumbers)%>%
    tm_map(removeWords,mystopword)%>%
    tm_map(stripWhitespace)
dtm<-DocumentTermMatrix(corpus.clean)

inspect(dtm[1:10,1:20])

df.train<-df[1:50,]
df.test<-df[51:100,]

dtm.train<-dtm[1:50,]
dtm.test<-dtm[51:100,]

corpus.clean.train<-corpus.clean[1:50]
corpus.clean.test<-corpus.clean[51:100]

dim(dtm.train)
fivefreq<-findFreqTerms(dtm.train,5)
length(fivefreq)

dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))

#dim(dtm.train.nb)

dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))

dim(dtm.test.nb)
 
convert_count <- function(x){
    y<-ifelse(x>0,1,0)
    y<-factor(y,levels=c(0,1),labels=c("no","yes"))
    y
}
trainNB<-apply(dtm.train.nb,2,convert_count)
testNB<-apply(dtm.test.nb,1,convert_count)


library(wordcloud)
wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))

```


```{r}
library(shiny) #package shiny
twitter <- read.csv(file="TweetCleanSquidGame.csv",header=TRUE)

#membuka text file pada data frame twitter
tweet <- twitter$text

# bagian yang mengatur tampilan web, baik input maupun outpun yang akan ditampilkan dalam web app.
ui <- fluidPage(
  titlePanel("Penggunaan Kata Squid Game Pada Twitter"), #halaman judul dr fluid page
  mainPanel( #tab pada fluidpage
    #plot output : untuk scatterplot
    tabsetPanel(type = "tabs",
                tabPanel("Scatterplot", plotOutput("scatterplot")), #tab berupa scatterplot/grafik
                tabPanel("Data Twitter", DT::dataTableOutput('tbl')), #tab berupa data clening twitter
                tabPanel("Wordcloud", plotOutput("Wordcloud")) #tab berupa worldcloud
    )
  )
)
# SERVER
# Disinialah tempat dimana data akan dianalisis dan diproses lalu hasilnya akan ditampilkan atau diplotkan pada bagian mainpanel() ui yang telah dijelaskan sebelumnya.
server <- function(input, output) {
  
  # Output Data
  output$tbl = DT::renderDataTable({ 
    DT::datatable(twitter, options = list(lengthChange = FALSE)) # data akan ditampilkan dalam beberapa halaman.
  })
  
  #Barplot
  output$scatterplot <- renderPlot({squidgame_dataset<-read.csv("TweetCleanSquidGame.csv",stringsAsFactors = FALSE)
  review <-as.character(squidgame_dataset$text)
  s<-get_nrc_sentiment(review)
  review_combine<-cbind(squidgame_dataset$text,s)
  par(mar=rep(3,4))
  barplot(colSums(s),col=rainbow(10),ylab='count',main='sentiment analisis')
  }, height=400)
  
  #WordCloud
  output$Wordcloud <- renderPlot({
    set.seed(20)
    df<-df[sample(nrow(df)),]
    df<-df[sample(nrow(df)),]
    glimpse(df)
    df$X=as.factor(df$X)
    corpus<-Corpus(VectorSource(df$text))
    corpus
    inspect(corpus[1:10])
    #fungsinya untuk membersihkan data data yang tidak dibutuhkan 
    corpus.clean<-corpus%>%
      tm_map(content_transformer(tolower))%>%
      tm_map(removePunctuation)%>%
      tm_map(removeNumbers)%>%
      tm_map(removeWords,stopwords(kind="en"))%>%
      tm_map(stripWhitespace)
    dtm<-DocumentTermMatrix(corpus.clean)
    inspect(dtm[1:10,1:20])

    df.train<-df[1:50,]
    df.test<-df[51:100,]

    dtm.train<-dtm[1:50,]
    dtm.test<-dtm[51:100,]

    corpus.clean.train<-corpus.clean[1:50]
    corpus.clean.test<-corpus.clean[51:100]
    
    dim(dtm.train)
    fivefreq<-findFreqTerms(dtm.train,5)
    length(fivefreq)
    dtm.train.nb<-DocumentTermMatrix(corpus.clean.train,control = list(dictionary=fivefreq))
    dtm.test.nb<-DocumentTermMatrix(corpus.clean.test,control = list(dictionary=fivefreq))
    dim(dtm.test.nb)
    convert_count <- function(x){
      y<-ifelse(x>0,1,0)
      y<-factor(y,levels=c(0,1),labels=c("no","yes"))
      y
    }
    trainNB<-apply(dtm.train.nb,2,convert_count)
    testNB<-apply(dtm.test.nb,1,convert_count)
    classifier<-naiveBayes(trainNB,df.train$X,laplace = 1)
    library(wordcloud)
    wordcloud(corpus.clean,min.freq = 4,max.words=100,random.order=F,colors=brewer.pal(8,"Dark2"))
  })
}
shinyApp(ui = ui, server = server)


```